{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization Algorithm\n",
    "\n",
    "Finally, we get to the most renowned optimizatio algorithm for HMMs: The Baum-Welch optimization/reparameterization algorithm.  Ultimately, this algorithm is an early-implemetaion of the relatively broad class of expectation-maximization algorithms.\n",
    "\n",
    "In this notebook, we go through, in details, the calculation of a single iteration of this procedure, and generalize the process to iteratively improve estimates until convergence is achieved.\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "Before discussing the optimization procedure in too much detail, there are a few numerical quantitites that we need in order to proceed with the EM algorithm. For the purpoises of conciseness they are just listed here, but I will reference the tutorial notebook that overviews each of these quantities in more detail.\n",
    "\n",
    "First, we will need the Bayesian estimate of the hidden state $p(x_t | Y^T)$ which is the best estimate of the hidden state, given information about all observations (past and future). This quantity is discussed in more detail in the notebook `03-slarge-hmm-filters.ipynb` as well as Ref.[1].\n",
    "\n",
    "Second, we will need the values of the forward and backward trajectory probabilities, denoted (as is convention) by $\\alpha_t(i) \\equiv p(Y^{t} | x_t=i)$ and $\\beta_t(j) \\equiv p(Y^{[t:T]}| x_t = j)$. Further discussion of these quantities is contained in notebook `04-slarge-alpha-beta.ipynb` as well as Ref.[2]\n",
    "\n",
    "Now, the actual implementation of the EM algorithm comes through the iterative application of expectation (E) and maximization (M) steps. Along with this iteration there is a convergence criterion, which will effectively determine how long to repeat the iteration as well as track the convergence of the model towards its local minimum.  To start, we discuss the actual calculation of the expectation and maximization steps in isolation.\n",
    "\n",
    "Ultimately, the differentiation of this method of attack as compared to the preivous likelihood calulations comes about by what we consider to be the likelihood function. Specifically, we previously used a likelihood function of the form $\\mathcal{L}(\\theta | Y^T)$ which effecively assumes that the *data* inquestion for this optimization are the observations. Conversely, we could take the stance that the true *data* for this problem is the set of all system states $Y$ *and* $X$ (hence our previous references to the likelihood as a *partial* likelihood function). In this sense, the true likelihood (or, as it is often called, the *Comnplete Data Likelihood Function*) $\\mathcal{L}(\\theta | Y^T, X^T)$ contains both the observations as well as the hidden states. However, because we do not know what the hidden states are, we cannot directly optimize this function. As it turns out, this is exactly where the EM algorithm shines, as the iterative approach will ensure that the full likelihood function will converge towards a global maximum upon iterative estimation (or re-estimation) of model parameters.\n",
    "\n",
    "### Expectation Step\n",
    "\n",
    "The expectation step is most simple, as it simply revolves around calculating the expected value of the hidden state sequence, given the available observation data, and an estiamte (initially a guess) at the dynamics matrices $A$ and $B$. Put simply, this is simply a calculation of the Bayesian state estimates $p(x_t | Y^T)$ using the current dynamics and observation matrices $\\boldsymbol{A}$ and $\\boldsymbol{B}$, respectively.\n",
    "\n",
    "### Minimizaion Step\n",
    "\n",
    "Following the expectation step, the maximizatioon step involves the updating of these matrices based on our best-guesses of inter-state transition probabilities of hidden states, as well as expected observation errors. To start with the former, note that we can, in general, estimate the rate of transition $i\\to j$ in a Markov model by computing the ratio\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\frac{N_{i\\to j}}{N_{i}} $$\n",
    "\n",
    "where $N_{i\\to j}$ is the number of observed transitions from $i \\to j$ and $N_i$ is the number of times the system was observed to be in state $i$. For a hidden Markov model, we can write down a probabilistic version of this equation as\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\frac{\\sum_t p(x_{j, t+1}, x_{i, t} | Y^T)}{\\sum_t p(x_{i, t} | Y^T)} $$\n",
    "\n",
    "where $p(x_{j, t+1}, x_{i, t} | Y^T)$ denotes the probability of transitioning from state $i\\to j$ during the $t\\to t+1$ timestep, conditioned upon all observations. Now, in order to calculate the numerator here though, we need to actually figure out how to estimate this probability. In supporting documentation, we show that this numerator term can be written as\n",
    "\n",
    "$$ p(x_{t, i}, x_{t-1, j} | Y^T) = \\frac{\\beta_{t}(i)\\alpha_{t-1}(j)A_{ij}}{\\sum_{i} \\beta_{t}(i)\\alpha_{t-1}(j)A_{ij}}p(x_{t-1, i} | Y^T) $$\n",
    "\n",
    "which is just a combination of several terms that we already know how to calcualte.\n",
    "\n",
    "Now, useing this result, we can *re-estimate* the parameters of the matrix $\\boldsymbol{A}$.\n",
    "\n",
    "To perform a similar update to the matrix $\\boldsymbol{B}$, we can perform a similar calculation (albeit much more simple). You can show (again, discussed in more detail in supporting documentation) that the update to elements of the $\\boldsymbol{B}$ matrix is\n",
    "\n",
    "$$ \\hat{B}_{ij} = \\frac{\\sum_t \\delta_{y_t, i} p(x_{t, j} | Y^T)}{\\sum_t p(x_{t, j} | Y^T)} $$\n",
    "\n",
    "Now, we go about building the necessary computational tools to actually determine how to perform these updates to the dynamics matrices, and show how this can lead to improved HMM optimization.\n",
    "\n",
    "#### References\n",
    "- [1] J. Bechhoefer *Control Theory of Physicists*, Cambridge University Press, 2020, Cambridge, MA\n",
    "- [2] *Numerical Recipes* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start we need to import the necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hidden import dynamics\n",
    "from hidden import infer\n",
    "\n",
    "# Declare sample (2D) HMM\n",
    "hmm = dynamics.HMM(2, 2)\n",
    "hmm.init_uniform_cycle(0.15, 0.2)\n",
    "hmm.run_dynamics(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate some initial 'guesses' at the A and B matrices, that are\n",
    "# close enugh to the true values\n",
    "A_init = np.array([\n",
    "    [0.9, 0.1],\n",
    "    [0.1, 0.9]\n",
    "])\n",
    "\n",
    "B_init = np.array([\n",
    "    [0.85, 0.25],\n",
    "    [0.15, 0.75]\n",
    "])\n",
    "\n",
    "# And pull off the vector of observations\n",
    "obs_ts = hmm.get_obs_ts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can calculate the bayesian estimates\n",
    "analyzer = infer.MarkovInfer(2, 2)\n",
    "analyzer.bayesian_smooth(obs_ts, A_init, B_init)\n",
    "bayes_est = analyzer.bayes_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, has essentially already completed the expectation step, so now we want\n",
    "# to look at the maximization procedure, for which we need both the alpha and\n",
    "# beta values\n",
    "analyzer.alpha(obs_ts, A_init, B_init)\n",
    "analyzer.beta(obs_ts, A_init, B_init)\n",
    "\n",
    "alpha_est = analyzer.alpha_tracker\n",
    "beta_est = analyzer.beta_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031576711905321256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have all of the ingredients for the calculation of the joint\n",
    "# probability term required to updte A.\n",
    "\n",
    "# Given then that alpha and beta terms are reported at t and t+1, we can only generate a value for t=0 -- T-1\n",
    "\n",
    "# Consider first the A_ii term, which will be given at t=1 as\n",
    "numer_00 = beta_est[1, 0] * alpha_est[0, 0] * A_init[0, 0]\n",
    "denom_00 = (beta_est[1, 0] * A_init[0, 0] + beta_est[1, 1] * A_init[0, 1]) * alpha_est[0, 0]\n",
    "joint_prob_00 = (numer_00 / denom_00) * bayes_est[0, 0]\n",
    "joint_prob_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182307812257245"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_A_00 = joint_prob_00 / bayes_est[0, 0]\n",
    "new_A_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9, 0.1],\n",
       "       [0.1, 0.9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004187039035102565"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, using this one data point, we would increase the 00 entry by ~0.1,\n",
    "# WE can also look at how the A_10 entry would update\n",
    "# NOTE for alpha terms alpha[t, state] and for A matrix: A[j, i]\n",
    "numer_10 = beta_est[1, 0] * alpha_est[0, 1] * A_init[1, 0]\n",
    "denom_10 = (beta_est[1, 0] * A_init[1, 0] + beta_est[1, 1] * A_init[1, 1]) * alpha_est[0, 1]\n",
    "joint_prob_10 = (numer_10 / denom_10) * bayes_est[0, 0]\n",
    "joint_prob_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12175644303157906"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_A_10 = joint_prob_10 / bayes_est[0, 0]\n",
    "new_A_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
