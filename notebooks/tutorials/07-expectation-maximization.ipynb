{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization Algorithm\n",
    "\n",
    "Finally, we get to the most renowned optimizatio algorithm for HMMs: The Baum-Welch optimization/reparameterization algorithm.  Ultimately, this algorithm is an early-implemetaion of the relatively broad class of expectation-maximization algorithms.\n",
    "\n",
    "Ultimately, the difference between the EM algorithm that we discuss here, and previous likelihood-maximizing algorithms is the definition of the 'data' that is used in the likelihood function.  Specifically, the previous likelihood function that we have maximized takes the general form $\\mathcal{L}(\\theta | Y^T)$ effectively taking the set of observed quantitites as the set of data for the optimization. However, another apporach considers the joint set of $Y$ *and* $X$ as the full set of data, and therefore effectively maximizes the likelihood $\\mathcal{L}(\\theta | X^T, Y^T)$. However, the issue is that this likelihood function can no longer be explicitly maximized, becuase we dont directly observe the hidden state sequence $X^T$.  This is why we have to use the EM algorithm (known as the Baum-Welch algorithm in the context of HMMs).\n",
    "\n",
    "Ultimately, this algorithm is going to give us an iterative method of updating the elements of the transition ($\\boldsymbol{A}$) and observation ($\\boldsymbol{B}$) matrices, in such a way that they will converge towards the minimum of the so-called *Complete Data Log Likelihood* $\\mathcal{L}(\\theta | X^T, Y^T)$.  Here, the difficult part is inferring the elements of the transition matrix $\\boldsymbol{A}$, which quantifies the transition rates between hidden states. If we were able to observe these states directly, then our best estimate of the $j\\to i$ transition rate $p(x_{i, t+1} | x_{t, j})$ would be\n",
    "\n",
    "$$ \\hat{A}_{ij} =  \\frac{N_{j \\to i}}{N_j} $$\n",
    "\n",
    "Which, we can restate as a probabilistic equation as\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\frac{\\sum_t p(x_{i, t}, x_{j, t-1} | Y^T)}{\\sum_t p(x_{j, t-1} | Y^T)} $$\n",
    "\n",
    "In fact, this exact equation is what the `Maximization' (M) step of the EM algorithm is doing in this case. So our goal is then first to figure out how to calculate the unknown probability terms on teh RHS above.  First, note that the term in the denominator is just the Bayesan probabiliy estimate, which we alreay know how to calcualte useing the Bayesian smoothing algorithm. The top term, however, is more tricky to determine.\n",
    "\n",
    "To start, note that we can use the law of total probability to split the joint probability into a product:\n",
    "\n",
    "$$ p(x_{i, t}, x_{j, t-1} | Y^T) = p(x_{i, t} | x_{j, t-1}, Y^T) p(x_{j, t-1} | Y^T) $$\n",
    "\n",
    "where again, the second term on the RHS is the Bayesian smoothed state estimate.\n",
    "\n",
    "Now, to deal with the first toer on the RHS of the equation above, we first make use of Bayes rule, which states for random variables $X$ and $Y$ that\n",
    "\n",
    "$$ P(X|Y) = \\frac{P(Y|X)P(X)}{\\sum_x P(Y|X)P(X)} $$\n",
    "\n",
    "and therefore we can rewrite the above probability term as\n",
    "\n",
    "$$ p(x_{i, t} | x_{j, t-1}, Y^T) = \\frac{p(Y^T | x_{i, t}, x_{j, t-1})p(x_{i, t} | x_{j, t-1})}{\\sum_{i} p(Y^T | x_{i, t}, x_{j, t-1})p(x_{i, t} | x_{j, t-1}) } $$\n",
    "\n",
    "Now, we can splmplify this a bit by noting that $p(x_{i, t} | x_{j, t-1}) = A_{ij}$.  Next, we make use of a property of HMMs known as conditional independence of observations, given the hiden states, which effectively means that, for a sequence of three hidden states, $x_1, x_2, x_3$ and corresponding observations $y_1, y_2, y_3$, we can split a conditional joint probability as\n",
    "\n",
    "$$ p(y_1, y_2, y_3 | x_1, x_2, x_3) = p(y_1 | x_1)p(y_2 | x_2)p(y_3 | x_3) $$\n",
    "\n",
    "Using this logic we can rewrite the first term in the RHS numerator above as\n",
    "\n",
    "$$\n",
    "p(Y^T | x_{i, t} x_{j, t-1}) = p(Y^{[0, t-1]}, Y^{[t, T]} | x_{i, t}, x_{j, t-1})  \\\\\n",
    "\\, \\\\\n",
    "= p(Y^{[0, t-1]} | x_{j, t-1})p(Y^{[t, T]} | x_{i, t}) \\\\\n",
    "\\, \\\\\n",
    "= p(Y^{[0, t-1]} | x_{j, t-1})p(Y_t | x_{i,t})p(Y^{[t+1, T]} | x_{i, t}) \\\\\n",
    "\\, \\\\\n",
    "= \\alpha_{t-1}(j) B_{i, Y_t} \\beta_t(j)\n",
    "$$\n",
    "\n",
    "and therefore we can rewrite our final expression for the joint probability as\n",
    "\n",
    "$$ p(x_{i, t}, x_{j, t-1} | Y^T) = \\frac{ \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} p(x_{j, t-1} | Y^T) }{\\sum_i \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} } $$\n",
    "\n",
    "And therefore we can estimate the terms in the $A$ matrix as\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\sum_t\\left[\\frac{\\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} p(x_{j, t-1} | Y^T) }{ \\sum_i \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} }\\right] \\frac{1}{\\sum_t p(x_{j, t-1} | Y^T)} $$\n",
    "\n",
    "The subtle thing here, is that in order to calcualte the terms on the RHS, we need to have an estimate already for $\\boldsymbol{A}$. This is ultimately where the iterative nature of the EM algorithm enters the picture.\n",
    "\n",
    "To start, we will go through this calculation and show how to run it using the `hidden` package before discussing the similar calcualte for the observation matrix $\\boldsymbol{B}$ (which turns out to be much simpler), and then finally more on to the actual implementation of the EM algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from hidden import dynamics, infer\n",
    "from hidden.filters import bayesian\n",
    "\n",
    "hmm = dynamics.HMM(2, 2)\n",
    "hmm.init_uniform_cycle()\n",
    "hmm.run_dynamics(200)\n",
    "\n",
    "obs_ts = hmm.get_obs_ts()\n",
    "\n",
    "analyzer = infer.MarkovInfer(2, 2)\n",
    "\n",
    "# Now start off with an estimate of the A and B matrices that is close to the true version\n",
    "A_perturb = np.array([\n",
    "    [0.1, 0.05],\n",
    "    [-0.1, -0.05]\n",
    "])\n",
    "\n",
    "B_perturb = np.array([\n",
    "    [0.06, -0.05],\n",
    "    [-0.06, 0.05]\n",
    "])\n",
    "\n",
    "A_init = hmm.A + A_perturb\n",
    "B_init = hmm.B + B_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start the calculation.  So, for this we need the bayesian filtered\n",
    "# probabilities, the alpha terms, and the beta terms\n",
    "\n",
    "analyzer.bayesian_smooth(obs_ts, A_init, B_init)\n",
    "analyzer.alpha(obs_ts, A_init, B_init, norm=True)\n",
    "analyzer.beta(obs_ts, A_init, B_init, norm=True)\n",
    "\n",
    "alpha_norm = analyzer.alpha_tracker\n",
    "beta_norm = analyzer.beta_tracker\n",
    "bayes = analyzer.bayes_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to get a single term (for a given time value), we can go over the\n",
    "# calcualtion element-wise\n",
    "\n",
    "# start with the 00 and 10 terms, which are for the 0 -> 0 and 1 -> 0 transitions\n",
    "\n",
    "sample_time = 10\n",
    "\n",
    "def get_numerator(i, j):\n",
    "    return (\n",
    "        A_init[i, j] * B_init[obs_ts[sample_time], i]\n",
    "        * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, i]\n",
    "        * bayes[sample_time - 1, j]\n",
    "    )\n",
    "\n",
    "def get_denom(j):\n",
    "    return (\n",
    "        (\n",
    "            A_init[0, j] * B_init[obs_ts[sample_time], 0]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 0]\n",
    "        )\n",
    "        + (\n",
    "            A_init[1, j] * B_init[obs_ts[sample_time], 1]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "numerator_00 = get_numerator(0, 0)\n",
    "numerator_10 = get_numerator(1, 0)\n",
    "numerator_01 = get_numerator(0, 1)\n",
    "numerator_11 = get_numerator(1, 1)\n",
    "\n",
    "# Now, the denominators are the same for both i values (across a column)\n",
    "\n",
    "# Used for A_00 and A_10\n",
    "denom_0 = get_denom(0)\n",
    "# Used for A_01 and A_11\n",
    "denom_1 = get_denom(1)\n",
    "\n",
    "# Now, if we only had this single point in time, then we can calcualte the entries\n",
    "\n",
    "A_00_new = (numerator_00 / denom_0) * (1 / bayes[sample_time - 1, 0])\n",
    "A_10_new = (numerator_10 / denom_0) * (1 / bayes[sample_time - 1, 0])\n",
    "A_01_new = (numerator_01 / denom_1) * (1 / bayes[sample_time - 1, 1])\n",
    "A_11_new = (numerator_11 / denom_1) * (1 / bayes[sample_time - 1, 1])\n",
    "\n",
    "A_new = np.array([\n",
    "    [A_00_new, A_01_new],\n",
    "    [A_10_new, A_11_new]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06675758, 0.00953759],\n",
       "       [0.93324242, 0.99046241]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And to see if it is still correctly normalized\n",
    "A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect.\n",
    "# So, now we can turn this into a procedure which will sum up observations over\n",
    "# the entire time series of observations\n",
    "\n",
    "# First we calculate the alpha, beta, and bayesian estimates\n",
    "analyzer.bayesian_smooth(obs_ts, A_init, B_init)\n",
    "analyzer.alpha(obs_ts, A_init, B_init, norm=True)\n",
    "analyzer.beta(obs_ts, A_init, B_init, norm=True)\n",
    "\n",
    "alpha_norm = analyzer.alpha_tracker\n",
    "beta_norm = analyzer.beta_tracker\n",
    "bayes = analyzer.bayes_smooth\n",
    "\n",
    "# Then we calculate the numeratiors and denominators in the same manner as above\n",
    "def get_numerator(i, j, sample_time):\n",
    "    return (\n",
    "        A_init[i, j] * B_init[i, obs_ts[sample_time]]\n",
    "        * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, i]\n",
    "        * bayes[sample_time - 1, j]\n",
    "    )\n",
    "\n",
    "def get_denom(j, sample_time):\n",
    "    return (\n",
    "        (\n",
    "            A_init[0, j] * B_init[0, obs_ts[sample_time]]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 0]\n",
    "        )\n",
    "        + (\n",
    "            A_init[1, j] * B_init[1, obs_ts[sample_time]]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "numer_00 = np.zeros(len(obs_ts) - 1)\n",
    "numer_10 = np.zeros_like(numer_00)\n",
    "numer_01 = np.zeros_like(numer_00)\n",
    "numer_11 = np.zeros_like(numer_00)\n",
    "\n",
    "denom_0 = np.zeros_like(numer_00)\n",
    "denom_1 = np.zeros_like(numer_00)\n",
    "\n",
    "for t in range(1, len(obs_ts)):\n",
    "    numer_00[t-1] = get_numerator(0, 0, t)\n",
    "    numer_10[t-1] = get_numerator(1, 0, t)\n",
    "    numer_01[t-1] = get_numerator(0, 1, t)\n",
    "    numer_11[t-1] = get_numerator(1, 1, t)\n",
    "\n",
    "    denom_0[t-1] = get_denom(0, t)\n",
    "    denom_1[t-1] = get_denom(1, t)\n",
    "\n",
    "A_00_new = sum(numer_00 / denom_0) * (1 / sum(bayes[:-1, 0]))\n",
    "A_10_new = sum(numer_10 / denom_0) * (1 / sum(bayes[:-1, 0]))\n",
    "A_01_new = sum(numer_01 / denom_1) * (1 / sum(bayes[:-1, 1]))\n",
    "A_11_new = sum(numer_11 / denom_1) * (1 / sum(bayes[:-1, 1]))\n",
    "    \n",
    "A_00_new_partial = sum(numer_00 / denom_0)\n",
    "A_10_new_partial = sum(numer_10 / denom_0)\n",
    "A_01_new_partial = sum(numer_01 / denom_1)\n",
    "A_11_new_partial = sum(numer_11 / denom_1)\n",
    "\n",
    "bayes_00_part = (1 / sum(bayes[:-1, 0]))\n",
    "bayes_10_part = (1 / sum(bayes[:-1, 0]))\n",
    "bayes_01_part = (1 / sum(bayes[:-1, 1]))\n",
    "bayes_11_part = (1 / sum(bayes[:-1, 1]))\n",
    "\n",
    "A_new = np.array([\n",
    "    [A_00_new, A_01_new],\n",
    "    [A_10_new, A_11_new]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.67850779, 0.35696576],\n",
       "        [0.32149221, 0.64303424]]),\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new, A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.12068016e-06, 5.05688458e-03],\n",
       "       [2.37169614e-05, 4.20118899e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2.12068016e-06, 5.05688458e-03],\n",
       "       [2.37169614e-05, 4.20118899e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To test the vectorization of a single loop, we can check the entries in the\n",
    "# matrix against single calls to the numeratir functions\n",
    "\n",
    "sample_00 = get_numerator(0, 0, sample_time=10)\n",
    "sample_10 = get_numerator(1, 0, sample_time=10)\n",
    "sample_01 = get_numerator(0, 1, sample_time=10)\n",
    "sample_11 = get_numerator(1, 1, sample_time=10)\n",
    "\n",
    "sample_A_mat = np.array([\n",
    "    [sample_00, sample_01],\n",
    "    [sample_10, sample_11]\n",
    "])\n",
    "\n",
    "sample_time = 10\n",
    "\n",
    "_B_mat = np.vstack([B_init[:, obs_ts[sample_time]], B_init[:, obs_ts[sample_time]]]).T\n",
    "_outer_term = np.outer(beta_norm[sample_time, :], (alpha_norm[sample_time - 1, :] * bayes[sample_time - 1, :]))\n",
    "sample_xi = A_init * _B_mat * _outer_term\n",
    "\n",
    "display(sample_xi)\n",
    "display(sample_A_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can clean this up a bit by vectorizing some of these calculations,\n",
    "# specifically, we organize all of the numerators into a matrix 'xi', we cant\n",
    "# really vectorize over time, but I think we can on a per-time-point basis\n",
    "\n",
    "def get_xi_matrix(obs, A, B, alpha_norm, beta_norm, bayes):\n",
    "    xi = np.zeros((2, 2, len(obs_ts) - 1))\n",
    "\n",
    "    for t in range(1, len(obs_ts)):\n",
    "        xi[:, :, t - 1] = (\n",
    "            A\n",
    "            * np.vstack([B[:, obs[t]], B[:, obs[t]]]).T\n",
    "            * np.outer(\n",
    "                beta_norm[t, :], (alpha_norm[t - 1, :] * bayes[t - 1, :])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return xi\n",
    "\n",
    "# The denominator for each element is just a column-wize normalization without\n",
    "# the bayes factor, so the calcualtion is very similar\n",
    "def get_denom_matrix(obs, A, B, alpha_norm, beta_norm):\n",
    "    xi_denom = np.zeros((1, 2, len(obs_ts) - 1))\n",
    "\n",
    "    for t in range(1, len(obs_ts)):\n",
    "        xi_denom[:, :, t-1] = np.sum(\n",
    "            A\n",
    "            * np.vstack([B[:, obs[t]], B[:, obs[t]]]).T\n",
    "            * np.outer(\n",
    "                beta_norm[t, :], alpha_norm[t - 1, :]\n",
    "            )\n",
    "        , axis=0)\n",
    "\n",
    "    # And then we sum over the columns\n",
    "    xi_denom = np.vstack([xi_denom, xi_denom])\n",
    "\n",
    "    return xi_denom\n",
    "\n",
    "xi_num = get_xi_matrix(obs_ts, A_init, B_init, alpha_norm, beta_norm, bayes)\n",
    "xi_denom = get_denom_matrix(obs_ts, A_init, B_init, alpha_norm, beta_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 199), (2, 2, 199))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_num.shape, xi_denom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.19081019e-01, 1.91065595e-05],\n",
       "       [2.43698698e-03, 6.66355298e-07]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There will be a 1-index shift here, because t=1 corresponds to index 0 (because we have t and t-1 in each loop)\n",
    "test_idx = 15\n",
    "\n",
    "xi_num[:, :, test_idx - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.19081019e-01, 1.91065595e-05],\n",
       "       [2.43698698e-03, 6.66355298e-07]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_audit_00 = get_numerator(0, 0, test_idx)\n",
    "A_audit_10 = get_numerator(1, 0, test_idx)\n",
    "A_audit_01 = get_numerator(0, 1, test_idx)\n",
    "A_audit_11 = get_numerator(1, 1, test_idx)\n",
    "\n",
    "A_audit = np.array([\n",
    "    [A_audit_00, A_audit_01],\n",
    "    [A_audit_10, A_audit_11]\n",
    "])\n",
    "A_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(A_audit, xi_num[:, :, test_idx - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52474129, 0.00321898],\n",
       "       [0.52474129, 0.00321898]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then also for the denominators\n",
    "xi_denom[:, :, test_idx - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52474129, 0.00321898],\n",
       "       [0.52474129, 0.00321898]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom_audit_0 = get_denom(0, test_idx)\n",
    "denom_audit_1 = get_denom(1, test_idx)\n",
    "\n",
    "denom_audit = np.array([\n",
    "    [denom_audit_0, denom_audit_1],\n",
    "    [denom_audit_0, denom_audit_1]\n",
    "])\n",
    "denom_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(denom_audit, xi_denom[:, :, test_idx - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 199)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, so it looks like the vectorized form is working, so now we should be\n",
    "# able to do an element-wise division of these matrices, to get the series of\n",
    "# numerical values for each matrix entry\n",
    "ratio = np.divide(xi_num, xi_denom)\n",
    "ratio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00993207, 0.01017128],\n",
       "       [0.00993207, 0.01017128]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then we just need to get the bayes matrix that we end up multiplying this\n",
    "# all by\n",
    "bayes_matrix = np.repeat((1 / bayes[:-1, :].sum(axis=0)).reshape(1, 2), 2, axis=0)\n",
    "bayes_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67850779, 0.35696576],\n",
       "       [0.32149221, 0.64303424]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, we should now be able to calcualte the whole updated matrix by performing\n",
    "# operations on the xi values\n",
    "\n",
    "A_new_xi = np.sum(ratio, axis=2) * (bayes_matrix)\n",
    "A_new_xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, everything looks to be working, and we can check this against the\n",
    "# explicit first pass update we have from the earler part\n",
    "\n",
    "np.isclose(A_new, A_new_xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this looks like it is working, so we can patch this all tegether for a\n",
    "# messy little algo to perform the updates to the A matrix\n",
    "\n",
    "def update_A_matrix(obs_ts, analyzer: infer.MarkovInfer, A_current, B_current):\n",
    "    analyzer.alpha(obs_ts, A_current, B_current, norm=True)\n",
    "    analyzer.beta(obs_ts, A_current, B_current, norm=True)\n",
    "    analyzer.bayesian_smooth(obs_ts, A_current, B_current)\n",
    "\n",
    "    xi_num = get_xi_matrix(\n",
    "        obs_ts, A_current, B_current,\n",
    "        analyzer.alpha_tracker,\n",
    "        analyzer.beta_tracker,\n",
    "        analyzer.bayes_smooth\n",
    "    )\n",
    "    xi_denom = get_denom_matrix(\n",
    "        obs_ts,\n",
    "        A_current,\n",
    "        B_current,\n",
    "        analyzer.alpha_tracker,\n",
    "        analyzer.beta_tracker\n",
    "    )\n",
    "\n",
    "    ratio = xi_num / xi_denom\n",
    "\n",
    "    bayes_matrix = np.repeat(\n",
    "        (1 / analyzer.bayes_smooth[:-1, :].sum(axis=0)).reshape(1, 2), 2,\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    new_A_matrix = np.sum(ratio, axis=2) * bayes_matrix\n",
    "    return new_A_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67850779, 0.35696576],\n",
       "       [0.32149221, 0.64303424]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And as a test to be sure this gets the same result\n",
    "\n",
    "update_A_matrix(obs_ts, analyzer, A_init, B_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67850779, 0.35696576],\n",
       "       [0.32149221, 0.64303424]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this working, we can move to how we would update the B matrix. So far we have effectively written out an implementation of the update equations for the transition matrix $\\boldsymbol{A}$, but not yet for $\\boldsymbol{B}$. For this, the actual update equation is relatively simple, we effectively want to calcualte terms like $p(y_{k, t}| x_{i, t})$. Probabilistically, we can write down an estimate for that value as\n",
    "\n",
    "$$ \\hat{B}_{ij} = \\hat{p}(y_{i, t} | x_{j, t}) = \\frac{\\sum_t \\delta_{y_t, i} p(x_{j, t} | Y^T ) }{\\sum_t p(x_{j, t} | Y^T ) }$$\n",
    "\n",
    "which is effectively the probability weighted number of observations of state $i$, relative to the probability of being in that state. So here, we just need the Bayesian estiamte, and to figure out how to programatically include the Kroneker-delta function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 200), (2, 2, 200))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_B_numer(i, j, obs, sample_time, bayes):\n",
    "    if obs != i:\n",
    "        return 0\n",
    "    \n",
    "    return bayes[sample_time, j]\n",
    "\n",
    "sample_time = 15\n",
    "\n",
    "# this is simple enough that we can probably just directly vectorize it\n",
    "gamma_mat = np.zeros((2, 2, len(obs_ts)))\n",
    "gamma_mat_denom = np.zeros_like(gamma_mat)\n",
    "\n",
    "for t, obs in enumerate(obs_ts):\n",
    "    gamma_mat[0, 0, t] = _get_B_numer(0, 0, obs, t, bayes)\n",
    "    gamma_mat[1, 0, t] = _get_B_numer(1, 0, obs, t, bayes)\n",
    "    gamma_mat[0, 1, t] = _get_B_numer(0, 1, obs, t, bayes)\n",
    "    gamma_mat[1, 1, t] = _get_B_numer(1, 1, obs, t, bayes)\n",
    "\n",
    "    gamma_mat_denom[0, 0, t] = bayes[t, 0]\n",
    "    gamma_mat_denom[1, 0, t] = bayes[t, 0]\n",
    "    gamma_mat_denom[0, 1, t] = bayes[t, 1]\n",
    "    gamma_mat_denom[1, 1, t] = bayes[t, 1]\n",
    "\n",
    "gamma_mat.shape, gamma_mat_denom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.17705414,  5.82294586],\n",
       "       [ 6.62913452, 93.37086548]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma_mat.sum(axis=2).sum(axis=0)\n",
    "gamma_mat.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.80618865,  99.19381135],\n",
       "       [100.80618865,  99.19381135]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_mat_denom.sum(axis=2)\n",
    "# gamma_mat_denom[:, :, 10]\n",
    "# gamma_mat_denom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93423881, 0.05870271],\n",
       "       [0.06576119, 0.94129729]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then to calcuqlate the updated B matrix:\n",
    "\n",
    "B_new_gamma = np.sum(gamma_mat, axis=2) / np.sum(gamma_mat_denom, axis=2)\n",
    "B_new_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_new_gamma.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[52.04904902, 52.33098106],\n",
       "        [47.95095098, 47.66901894]]),\n",
       " array([[100.80618865,  99.19381135],\n",
       "        [100.80618865,  99.19381135]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To clean this up more and make this routine a little more simplified, we\n",
    "# can vectorize the whole think\n",
    "\n",
    "ind_matrix = np.zeros((2, 2, len(obs_ts)))\n",
    "obs_ts_np = np.array(obs_ts)\n",
    "\n",
    "# Indicator matrix for each observation value\n",
    "ind_matrix[0, :, obs_ts_np == 0] = 1\n",
    "ind_matrix[1, :, obs_ts_np == 1] = 1\n",
    "\n",
    "gamma_mat = ind_matrix * np.vstack([bayes, bayes]).reshape(ind_matrix.shape)\n",
    "gamma_denom = np.vstack([bayes, bayes]).reshape(ind_matrix.shape)\n",
    "\n",
    "gamma_mat.sum(axis=2), gamma_mat_denom.sum(axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_matrix[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so this now works as well. So, taking all of this tegether, we can \n",
    "# write out a rough function that will perform a single iteration of the BW\n",
    "# algorithm\n",
    "\n",
    "# We will also re-declare all necessary functions that we have built so far\n",
    "# in this notebook here.\n",
    "\n",
    "def _update_A_matrix():\n",
    "    pass\n",
    "\n",
    "def _update_B_matrix():\n",
    "    pass\n",
    "\n",
    "def baum_welch_step(A_current, B_current, obs_ts):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
