{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization Algorithm\n",
    "\n",
    "Finally, we get to the most renowned optimizatio algorithm for HMMs: The Baum-Welch optimization/reparameterization algorithm.  Ultimately, this algorithm is an early-implemetaion of the relatively broad class of expectation-maximization algorithms.\n",
    "\n",
    "Ultimately, the difference between the EM algorithm that we discuss here, and previous likelihood-maximizing algorithms is the definition of the 'data' that is used in the likelihood function.  Specifically, the previous likelihood function that we have maximized takes the general form $\\mathcal{L}(\\theta | Y^T)$ effectively taking the set of observed quantitites as the set of data for the optimization. However, another apporach considers the joint set of $Y$ *and* $X$ as the full set of data, and therefore effectively maximizes the likelihood $\\mathcal{L}(\\theta | X^T, Y^T)$. However, the issue is that this likelihood function can no longer be explicitly maximized, becuase we dont directly observe the hidden state sequence $X^T$.  This is why we have to use the EM algorithm (known as the Baum-Welch algorithm in the context of HMMs).\n",
    "\n",
    "Ultimately, this algorithm is going to give us an iterative method of updating the elements of the transition ($\\boldsymbol{A}$) and observation ($\\boldsymbol{B}$) matrices, in such a way that they will converge towards the minimum of the so-called *Complete Data Log Likelihood* $\\mathcal{L}(\\theta | X^T, Y^T)$.  Here, the difficult part is inferring the elements of the transition matrix $\\boldsymbol{A}$, which quantifies the transition rates between hidden states. If we were able to observe these states directly, then our best estimate of the $j\\to i$ transition rate $p(x_{i, t+1} | x_{t, j})$ would be\n",
    "\n",
    "$$ \\hat{A}_{ij} =  \\frac{N_{j \\to i}}{N_j} $$\n",
    "\n",
    "Which, we can restate as a probabilistic equation as\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\frac{\\sum_t p(x_{i, t}, x_{j, t-1} | Y^T)}{\\sum_t p(x_{j, t-1} | Y^T)} $$\n",
    "\n",
    "In fact, this exact equation is what the `Maximization' (M) step of the EM algorithm is doing in this case. So our goal is then first to figure out how to calculate the unknown probability terms on teh RHS above.  First, note that the term in the denominator is just the Bayesan probabiliy estimate, which we alreay know how to calcualte useing the Bayesian smoothing algorithm. The top term, however, is more tricky to determine.\n",
    "\n",
    "To start, note that we can use the law of total probability to split the joint probability into a product:\n",
    "\n",
    "$$ p(x_{i, t}, x_{j, t-1} | Y^T) = p(x_{i, t} | x_{j, t-1}, Y^T) p(x_{j, t-1} | Y^T) $$\n",
    "\n",
    "where again, the second term on the RHS is the Bayesian smoothed state estimate.\n",
    "\n",
    "Now, to deal with the first toer on the RHS of the equation above, we first make use of Bayes rule, which states for random variables $X$ and $Y$ that\n",
    "\n",
    "$$ P(X|Y) = \\frac{P(Y|X)P(X)}{\\sum_x P(Y|X)P(X)} $$\n",
    "\n",
    "and therefore we can rewrite the above probability term as\n",
    "\n",
    "$$ p(x_{i, t} | x_{j, t-1}, Y^T) = \\frac{p(Y^T | x_{i, t}, x_{j, t-1})p(x_{i, t} | x_{j, t-1})}{\\sum_{i} p(Y^T | x_{i, t}, x_{j, t-1})p(x_{i, t} | x_{j, t-1}) } $$\n",
    "\n",
    "Now, we can splmplify this a bit by noting that $p(x_{i, t} | x_{j, t-1}) = A_{ij}$.  Next, we make use of a property of HMMs known as conditional independence of observations, given the hiden states, which effectively means that, for a sequence of three hidden states, $x_1, x_2, x_3$ and corresponding observations $y_1, y_2, y_3$, we can split a conditional joint probability as\n",
    "\n",
    "$$ p(y_1, y_2, y_3 | x_1, x_2, x_3) = p(y_1 | x_1)p(y_2 | x_2)p(y_3 | x_3) $$\n",
    "\n",
    "Using this logic we can rewrite the first term in the RHS numerator above as\n",
    "\n",
    "$$\n",
    "p(Y^T | x_{i, t} x_{j, t-1}) = p(Y^{[0, t-1]}, Y^{[t, T]} | x_{i, t}, x_{j, t-1})  \\\\\n",
    "\\, \\\\\n",
    "= p(Y^{[0, t-1]} | x_{j, t-1})p(Y^{[t, T]} | x_{i, t}) \\\\\n",
    "\\, \\\\\n",
    "= p(Y^{[0, t-1]} | x_{j, t-1})p(Y_t | x_{i,t})p(Y^{[t+1, T]} | x_{i, t}) \\\\\n",
    "\\, \\\\\n",
    "= \\alpha_{t-1}(j) B_{i, Y_t} \\beta_t(j)\n",
    "$$\n",
    "\n",
    "and therefore we can rewrite our final expression for the joint probability as\n",
    "\n",
    "$$ p(x_{i, t}, x_{j, t-1} | Y^T) = \\frac{ \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} p(x_{j, t-1} | Y^T) }{\\sum_i \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} } $$\n",
    "\n",
    "And therefore we can estimate the terms in the $A$ matrix as\n",
    "\n",
    "$$ \\hat{A}_{ij} = \\sum_t\\left[\\frac{\\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} p(x_{j, t-1} | Y^T) }{ \\sum_i \\alpha_{t-1}(j)\\beta_t(i) B_{i, y_t} A_{ij} }\\right] \\frac{1}{\\sum_t p(x_{j, t-1} | Y^T)} $$\n",
    "\n",
    "The subtle thing here, is that in order to calcualte the terms on the RHS, we need to have an estiamte already for $\\boldsymbol{A}$. This is ultimately where the iterative nature of the EM algorithm enters the picture.\n",
    "\n",
    "To start, we will go through this calculation and show how to run it using the `hidden` package before discussing the similar calcualte for the observation matrix $\\boldsymbol{B}$ (which turns out to be much simpler), and then finally more on to the actual implementation of the EM algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from hidden import dynamics, infer\n",
    "from hidden.filters import bayesian\n",
    "\n",
    "hmm = dynamics.HMM(2, 2)\n",
    "hmm.init_uniform_cycle()\n",
    "hmm.run_dynamics(200)\n",
    "\n",
    "obs_ts = hmm.get_obs_ts()\n",
    "\n",
    "analyzer = infer.MarkovInfer(2, 2)\n",
    "\n",
    "# Now start off with an estimate of the A and B matrices that is close to the true version\n",
    "A_perturb = np.array([\n",
    "    [0.1, 0.05],\n",
    "    [-0.1, -0.05]\n",
    "])\n",
    "\n",
    "B_perturb = np.array([\n",
    "    [0.06, -0.05],\n",
    "    [-0.06, 0.05]\n",
    "])\n",
    "\n",
    "A_init = hmm.A + A_perturb\n",
    "B_init = hmm.B + B_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can start the calculation.  So, for this we need the bayesian filtered\n",
    "# probabilities, the alpha terms, and the beta terms\n",
    "\n",
    "analyzer.bayesian_smooth(obs_ts, A_init, B_init)\n",
    "analyzer.alpha(obs_ts, A_init, B_init, norm=True)\n",
    "analyzer.beta(obs_ts, A_init, B_init, norm=True)\n",
    "\n",
    "alpha_norm = analyzer.alpha_tracker\n",
    "beta_norm = analyzer.beta_tracker\n",
    "bayes = analyzer.bayes_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to get a single term (for a given time value), we can go over the\n",
    "# calcualtion element-wise\n",
    "\n",
    "# start with the 00 and 10 terms, which are for the 0 -> 0 and 1 -> 0 transitions\n",
    "\n",
    "sample_time = 10\n",
    "\n",
    "def get_numerator(i, j):\n",
    "    return (\n",
    "        A_init[i, j] * B_init[obs_ts[sample_time], i]\n",
    "        * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, i]\n",
    "        * bayes[sample_time - 1, j]\n",
    "    )\n",
    "\n",
    "def get_denom(j):\n",
    "    return (\n",
    "        (\n",
    "            A_init[0, j] * B_init[obs_ts[sample_time], 0]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 0]\n",
    "        )\n",
    "        + (\n",
    "            A_init[1, j] * B_init[obs_ts[sample_time], 1]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "numerator_00 = get_numerator(0, 0)\n",
    "numerator_10 = get_numerator(1, 0)\n",
    "numerator_01 = get_numerator(0, 1)\n",
    "numerator_11 = get_numerator(1, 1)\n",
    "\n",
    "# Now, the denominators are the same for both i values (across a column)\n",
    "\n",
    "# Used for A_00 and A_10\n",
    "denom_0 = get_denom(0)\n",
    "# Used for A_01 and A_11\n",
    "denom_1 = get_denom(1)\n",
    "\n",
    "# Now, if we only had this single point in time, then we can calcualte the entries\n",
    "\n",
    "A_00_new = (numerator_00 / denom_0) * (1 / bayes[sample_time - 1, 0])\n",
    "A_10_new = (numerator_10 / denom_0) * (1 / bayes[sample_time - 1, 0])\n",
    "A_01_new = (numerator_01 / denom_1) * (1 / bayes[sample_time - 1, 1])\n",
    "A_11_new = (numerator_11 / denom_1) * (1 / bayes[sample_time - 1, 1])\n",
    "\n",
    "A_new = np.array([\n",
    "    [A_00_new, A_01_new],\n",
    "    [A_10_new, A_11_new]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96916984, 0.80885895],\n",
       "       [0.03083016, 0.19114105]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And to see if it is still correctly normalized\n",
    "A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect.\n",
    "# So, now we can turn this into a procedure which will sum up observations over\n",
    "# the entire time series of observations\n",
    "\n",
    "# First we calculate the alpha, beta, and bayesian estimates\n",
    "analyzer.bayesian_smooth(obs_ts, A_init, B_init)\n",
    "analyzer.alpha(obs_ts, A_init, B_init, norm=True)\n",
    "analyzer.beta(obs_ts, A_init, B_init, norm=True)\n",
    "\n",
    "alpha_norm = analyzer.alpha_tracker\n",
    "beta_norm = analyzer.beta_tracker\n",
    "bayes = analyzer.bayes_smooth\n",
    "\n",
    "# Then we calculate the numeratiors and denominators in the same manner as above\n",
    "def get_numerator(i, j, sample_time):\n",
    "    return (\n",
    "        A_init[i, j] * B_init[i, obs_ts[sample_time]]\n",
    "        * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, i]\n",
    "        * bayes[sample_time - 1, j]\n",
    "    )\n",
    "\n",
    "def get_denom(j, sample_time):\n",
    "    return (\n",
    "        (\n",
    "            A_init[0, j] * B_init[0, obs_ts[sample_time]]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 0]\n",
    "        )\n",
    "        + (\n",
    "            A_init[1, j] * B_init[1, obs_ts[sample_time]]\n",
    "            * alpha_norm[sample_time - 1, j] * beta_norm[sample_time, 1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "numer_00 = np.zeros(len(obs_ts) - 1)\n",
    "numer_10 = np.zeros_like(numer_00)\n",
    "numer_01 = np.zeros_like(numer_00)\n",
    "numer_11 = np.zeros_like(numer_00)\n",
    "\n",
    "denom_0 = np.zeros_like(numer_00)\n",
    "denom_1 = np.zeros_like(numer_00)\n",
    "\n",
    "for t in range(1, len(obs_ts)):\n",
    "    numer_00[t-1] = get_numerator(0, 0, t)\n",
    "    numer_10[t-1] = get_numerator(1, 0, t)\n",
    "    numer_01[t-1] = get_numerator(0, 1, t)\n",
    "    numer_11[t-1] = get_numerator(1, 1, t)\n",
    "\n",
    "    denom_0[t-1] = get_denom(0, t)\n",
    "    denom_1[t-1] = get_denom(1, t)\n",
    "\n",
    "A_00_new = sum(numer_00 / denom_0) * (1 / sum(bayes[:-1, 0]))\n",
    "A_10_new = sum(numer_10 / denom_0) * (1 / sum(bayes[:-1, 0]))\n",
    "A_01_new = sum(numer_01 / denom_1) * (1 / sum(bayes[:-1, 1]))\n",
    "A_11_new = sum(numer_11 / denom_1) * (1 / sum(bayes[:-1, 1]))\n",
    "    \n",
    "A_00_new_partial = sum(numer_00 / denom_0)\n",
    "A_10_new_partial = sum(numer_10 / denom_0)\n",
    "A_01_new_partial = sum(numer_01 / denom_1)\n",
    "A_11_new_partial = sum(numer_11 / denom_1)\n",
    "\n",
    "bayes_00_part = (1 / sum(bayes[:-1, 0]))\n",
    "bayes_10_part = (1 / sum(bayes[:-1, 0]))\n",
    "bayes_01_part = (1 / sum(bayes[:-1, 1]))\n",
    "bayes_11_part = (1 / sum(bayes[:-1, 1]))\n",
    "\n",
    "A_new = np.array([\n",
    "    [A_00_new, A_01_new],\n",
    "    [A_10_new, A_11_new]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.76114956, 0.30559352],\n",
       "        [0.23885044, 0.69440648]]),\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new, A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96, 0.05],\n",
       "       [0.96, 0.05]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([B_init[0, :], B_init[0, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can clean this up a bit by vectorizing some of these calculations,\n",
    "# specifically, we organize all of the numerators into a matrix 'xi', we cant\n",
    "# really vectorize over time, but I think we can on a per-time-point basis\n",
    "\n",
    "def get_xi_matrix(obs, A, B, alpha_norm, beta_norm, bayes):\n",
    "    xi = np.zeros((2, 2, len(obs_ts) - 1))\n",
    "\n",
    "    for t in range(1, len(obs_ts)):\n",
    "        xi[:, :, t - 1] = (\n",
    "            A\n",
    "            * np.vstack([B[obs[t], :], B[obs[t], :]]).T\n",
    "            # * np.hstack([B[obs[t], :], B[obs[t], :]])\n",
    "            * np.outer(\n",
    "                beta_norm[t, :], (alpha_norm[t - 1, :] * bayes[t - 1, :])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return xi\n",
    "\n",
    "# The denominator for each element is just a column-wize normalization without\n",
    "# the bayes factor, so the calcualtion is very similar\n",
    "def get_denom_matrix(obs, A, B, alpha_norm, beta_norm):\n",
    "    xi_denom = np.zeros((1, 2, len(obs_ts) - 1))\n",
    "\n",
    "    for t in range(1, len(obs_ts)):\n",
    "        xi_denom[:, :, t-1] = np.sum(\n",
    "            A\n",
    "            # * np.repeat(\n",
    "                # B[obs[t], :].reshape(1, A.shape[0]),\n",
    "                # 2, axis=0\n",
    "            # )\n",
    "            * np.vstack([B[obs[t], :], B[obs[t], :]]).T\n",
    "            * np.outer(\n",
    "                beta_norm[t, :], alpha_norm[t - 1, :]\n",
    "            )\n",
    "        , axis=0)\n",
    "\n",
    "    # And then we sum over the columns\n",
    "    xi_denom = np.vstack([xi_denom, xi_denom])\n",
    "\n",
    "    return xi_denom\n",
    "\n",
    "xi_num = get_xi_matrix(obs_ts, A_init, B_init, alpha_norm, beta_norm, bayes)\n",
    "xi_denom = get_denom_matrix(obs_ts, A_init, B_init, alpha_norm, beta_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 199), (2, 2, 199))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_num.shape, xi_denom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.27925203e-05, 4.62673302e-03],\n",
       "       [1.48795671e-03, 6.17701900e-01]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_num[:, :, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.27925203e-05, 4.62673302e-03],\n",
       "       [1.48795671e-03, 6.17701900e-01]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_num[:, :, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.279252026599447e-05,\n",
       " 0.6177019002280281,\n",
       " 0.004626733021507513,\n",
       " 0.0014879567121813487)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_numerator(0, 0, 10), get_numerator(1, 1, 10), get_numerator(0, 1, 10), get_numerator(1, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02251799, 0.39778689],\n",
       "       [0.02251799, 0.39778689]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_denom[:, :, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0225179929378272, 0.39778688513214056)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_denom(0, 10), get_denom(1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2334544270044038, 0.0013099873226974951)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_denom(0, 10), get_denom(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 199)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we should be able to do an element-wise division of these matrices,\n",
    "# to get the series of numerical values for each matrix entry\n",
    "ratio = np.divide(xi_num, xi_denom)\n",
    "ratio.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03849123e-03, 1.52472458e-02],\n",
       "       [1.50652224e-02, 1.64312053e+00]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio[:,  :, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[84.24136389, 46.63270354],\n",
       "       [31.83783815, 85.85199759]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ratio, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84.24136388656765, 46.63270354203865, 31.83783815140865, 85.85199758862484)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_00_new_partial, A_01_new_partial, A_10_new_partial, A_11_new_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00861481, 0.00754804],\n",
       "       [0.00861481, 0.00754804]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then we just need to get the bayes matrix that we end up multiplying this\n",
    "# all by\n",
    "bayes_matrix = np.repeat((1 / bayes[:-1, :].sum(axis=0)).reshape(1, 2), 2, axis=0)\n",
    "bayes_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008614807669618899, 0.007548041332061026)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_00_part, bayes_01_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72572315, 0.35198557],\n",
       "       [0.27427685, 0.64801443]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, we should now be able to calcualte the whole updated matrix by performing\n",
    "# operations on the xi values\n",
    "\n",
    "A_new = np.sum(ratio, axis=2) * (bayes_matrix)\n",
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this looks like it is working,\n",
    "\n",
    "# HOWEVER the actual bayesian filter does not appear to be working anymore... for some reason...\n",
    "# so ill dig into that now, but we can quickly patch together a v1 implementation of the 'update_a_matrix' routine\n",
    "\n",
    "def update_A_matrix(obs_ts, analyzer: infer.MarkovInfer, A_current, B_current):\n",
    "    analyzer.alpha(obs_ts, A_current, B_current, norm=True)\n",
    "    analyzer.beta(obs_ts, A_current, B_current, norm=True)\n",
    "    analyzer.bayesian_smooth(obs_ts, A_current, B_current)\n",
    "\n",
    "    xi_num = get_xi_matrix(\n",
    "        obs_ts, A_current, B_current,\n",
    "        analyzer.alpha_tracker,\n",
    "        analyzer.beta_tracker,\n",
    "        analyzer.bayes_smooth\n",
    "    )\n",
    "    xi_denom = get_denom_matrix(\n",
    "        obs_ts,\n",
    "        A_current,\n",
    "        B_current,\n",
    "        analyzer.alpha_tracker,\n",
    "        analyzer.beta_tracker\n",
    "    )\n",
    "\n",
    "    ratio = xi_num / xi_denom\n",
    "\n",
    "    bayes_matrix = np.repeat(\n",
    "        (1 / analyzer.bayes_smooth[:-1, :].sum(axis=0)).reshape(1, 2), 2,\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    new_A_matrix = np.sum(ratio, axis=2) * bayes_matrix\n",
    "    return new_A_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72572315, 0.35198557],\n",
       "       [0.27427685, 0.64801443]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And as a test to be sure this gets the same result\n",
    "\n",
    "update_A_matrix(obs_ts, analyzer, A_init, B_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72572315, 0.35198557],\n",
       "       [0.27427685, 0.64801443]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
