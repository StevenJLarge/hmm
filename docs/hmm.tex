\documentclass{article}
%\usepackage[utf8]{inputenc}

\usepackage{slarge_template}
\newcommand{\bs}{\boldsymbol}

\title{\tga Hidden Markov Models}
\author{\bf \tga Steven J Large}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\pagebreak

\twocolumn
\sloppy

\section{Introduction}

Hidden Markov Models (HMMs) represent a class of models that provide a convenient (and often necessary) mechanism of integrating measurement and observation noise/errors into the scope of a theoretical description. HMMs have found atremendous level of applicability to many problems in the word of information theory, signal processing, as well as---more recently---within the machine learning/artificial intelligence community. Concretely, a hidden markov model is just a probabilistic function of a Markov model. Put more simply, the underlying model (whatever that may be) is assumed to be Markovian, but the observation channel translates that Markovian process into an output (or \emph{observation}) sequence that contains some amount of noise.~\footnote{Interestingly (and importantly) the observed sequence of states is \emph{not necessarily} Markovian}

% Example -- Show figure etc.

% Discuss implications for finance? (and history)

\section{Discrete State Markov Models}

\section{Hidden Markov Models}

\subsection{Bayesian Filtering}

\subsection{Backward Algorithm}

\subsection{Bayesian Smoothing}

\section{Parameter Inferrence}

\subsection{Likelihood Maximization}

\subsection{Baum-Welch Rearameterization}

\section{Discussion}

\section{Conclusions}

\appendix



\end{document}